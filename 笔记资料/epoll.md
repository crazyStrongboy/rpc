### 核心API

#### epoll_create

创建一个file节点，后续socket都会与这个节点有关联。有几个重要属性：红黑树rbr，就绪链表rdllist

#### epoll_ctl

增加socket句柄时，加入到红黑树中，并会向内核注册回调函数，用于中断事件来向准备就绪链表插入数据。

#### epoll_wait

返回准备就绪链表里的数据即可。

#### ep_poll_callback

触发该函数将事件添加到就绪列表中。



### ET于LT模式

#### ET模式

边缘触发，只会通知一次，如果没有读取全部数据，将导致epoll不会再通知该socket的read事件。ET模式下只有当socket状态发生改变时才会进行通知。

#### LT模式

水平模式，read缓冲区还有数据或者write缓冲区为空时，则每次调用`epoll_wait`都会返回它的事件，提醒用户程序去操作。



**一道面试题**

使用Linux epoll模型的LT水平触发模式，当socket可写时，会不停的触发socket可写的事件，如何处理？

**普通做法**：

当需要向socket写数据时，将该socket加入到epoll等待可写事件。接收到socket可写事件后，调用write()或send()发送数据，当数据全部写完后， 将socket描述符移出epoll列表，这种做法需要反复添加和删除。

**改进做法**:

向socket写数据时直接调用send()发送，当send()返回错误码EAGAIN，才将socket加入到epoll，等待可写事件后再发送数据，全部数据发送完毕，再移出epoll模型，改进的做法相当于认为socket在大部分时候是可写的，不能写了再让epoll帮忙监控。

上面两种做法是对LT模式下write事件频繁通知的修复，本质上ET模式就可以直接搞定，并不需要用户层程序的补丁操作。



### epoll惊群问题

多个线程同时监听同一个fd时，当内核触发可读写事件，多个线程都会同时相应，但实际上只有一个线程去真实的处理这个问题。在epoll官方没有正式修复这个问题之前，Nginx作为知名使用者采用全局锁来限制每次可监听fd的进程数量，每次只有1个可监听的进程，后来在Linux 3.9内核中增加了SO_REUSEPORT选项实现了内核级的负载均衡，Nginx1.9.1版本支持了reuseport这个新特性，从而解决惊群问题。



### Socket缓冲区

**每一个socket被创建之后，系统都会给它们分配两个缓冲区，即输入缓冲区和输出缓冲区。**

1. send函数并不是直接将数据传输到网络中，而是负责将数据写入输出缓冲区，数据从输出缓冲区发送到目标主机是由TCP协议完成的。数据写入到输出缓冲区之后，send函数就可以返回了，数据是否发送出去，是否发送成功，何时到达目标主机，都不由它负责了，而是由协议负责。

2. recv函数也是一样的，它并不是直接从网络中获取数据，而是从输入缓冲区中读取数据。

